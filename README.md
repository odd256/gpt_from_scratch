# Introduction

This repository contains the code for the paper "GPT-2: Language Models are Unsupervised Multitask Learners" by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. The code is based on the official implementation by OpenAI and is modified to work with the PyTorch framework.

- The code referred to this video (strongly recommended): [Let's reproduce GPT-2 (124M)](https://youtu.be/l8pRSuU81PU?si=l791ptIHZPT99pTz)

- The Chinese notes for GPT-2 reproduction: [从零复现GPT2](https://odd256.github.io/Machine-Learning/Large-Language-Model/%E4%BB%8E%E9%9B%B6%E5%A4%8D%E7%8E%B0GPT2)

# How to use

This Project is managed by `uv`, so you can use the following command to install the dependencies:

1. `git clone this repo`
2. `cd gpt_from_scratch`
3. `uv sync`
